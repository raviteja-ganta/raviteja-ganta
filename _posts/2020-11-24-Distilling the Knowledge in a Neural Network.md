---
layout: post
published: true
title: Distilling the Knowledge in a Neural Network
---
 

How can we compress and transfer knowledge from bigger model or ensemble of models(which were trained on very large datasets to extract structure from data) to a single small
model with out much dip in performance?


But why do we want to do this? Why we need smaller model when bigger model or ensemble model is already giving great results on test data?


<p align="center">
  <img src="https://raw.githubusercontent.com/raviteja-ganta/raviteja-ganta.github.io/master/images/Distill_knowledge/dk_1.png" />
</p>

